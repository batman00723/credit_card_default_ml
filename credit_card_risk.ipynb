{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f72984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_renamed = X.copy()\n",
    "\n",
    "X_renamed.columns = [\n",
    "    \"credit_limit\",        # X1\n",
    "    \"gender\",              # X2\n",
    "    \"education\",           # X3\n",
    "    \"marital_status\",      # X4\n",
    "    \"age\",                 # X5\n",
    "\n",
    "    # Repayment status (most recent â†’ oldest)\n",
    "    \"pay_sep\",             # X6\n",
    "    \"pay_aug\",             # X7\n",
    "    \"pay_jul\",             # X8\n",
    "    \"pay_jun\",             # X9\n",
    "    \"pay_may\",             # X10\n",
    "    \"pay_apr\",             # X11\n",
    "\n",
    "    # Bill amounts\n",
    "    \"bill_sep\",            # X12\n",
    "    \"bill_aug\",            # X13\n",
    "    \"bill_jul\",            # X14\n",
    "    \"bill_jun\",            # X15\n",
    "    \"bill_may\",            # X16\n",
    "    \"bill_apr\",            # X17\n",
    "\n",
    "    # Payment amounts\n",
    "    \"pay_amt_sep\",         # X18\n",
    "    \"pay_amt_aug\",         # X19\n",
    "    \"pay_amt_jul\",         # X20\n",
    "    \"pay_amt_jun\",         # X21\n",
    "    \"pay_amt_may\",         # X22\n",
    "    \"pay_amt_apr\"          # X23\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_cols = [\n",
    "    \"pay_sep\", \"pay_aug\", \"pay_jul\",\n",
    "    \"pay_jun\", \"pay_may\", \"pay_apr\"\n",
    "]\n",
    "\n",
    "X_feat = X_renamed.copy()\n",
    "\n",
    "# Features \n",
    "X_feat[\"recent_delay\"] = X_feat[\"pay_sep\"]\n",
    "\n",
    "X_feat[\"max_delay\"] = X_feat[pay_cols].max(axis=1)\n",
    "# axis =1 as we need customer wise aggregation\n",
    "X_feat[\"mean_delay\"] = (X_feat[pay_cols].clip(lower=0).mean(axis=1))\n",
    "# Here we used clip() as we need to focus only on delay payments thats how banks work\n",
    "\n",
    "X_feat['num_delayed_months'] = (X_feat[pay_cols] > 0).sum(axis=1)\n",
    "# Out of 6 months, how many times was the customer late?\n",
    "\n",
    "X_feat[\"any_severe_delay\"] = (X_feat[\"max_delay\"] >= 3).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bill and Payment Features\n",
    "\n",
    "bill_cols = [\n",
    "    \"bill_sep\", \"bill_aug\", \"bill_jul\",\n",
    "    \"bill_jun\", \"bill_may\", \"bill_apr\"\n",
    "]\n",
    "\n",
    "pay_amt_cols = [\n",
    "    \"pay_amt_sep\", \"pay_amt_aug\", \"pay_amt_jul\",\n",
    "    \"pay_amt_jun\", \"pay_amt_may\", \"pay_amt_apr\"\n",
    "]\n",
    "\n",
    "X_feat[\"avg_bill\"] = X_feat[bill_cols].mean(axis=1)\n",
    "X_feat[\"avg_payment\"] = X_feat[pay_amt_cols].mean(axis=1)\n",
    "\n",
    "total_bill = X_feat[bill_cols].sum(axis=1)\n",
    "total_payment = X_feat[pay_amt_cols].sum(axis=1)\n",
    "\n",
    "\n",
    "# Here we will use clip(upper=2) cus we dont want our model to explode and anything above those can be treated same.\n",
    "X_feat[\"payment_ratio\"] = (\n",
    "    total_payment / total_bill.replace(0, 1)\n",
    ").clip(upper=2)\n",
    "\n",
    "X_feat[\"utilization_ratio\"] = (\n",
    "    X_feat[\"avg_bill\"] / X_feat[\"credit_limit\"].replace(0, 1)\n",
    ").clip(upper=2)\n",
    "\n",
    "X_feat[\"recent_payment_ratio\"] = (\n",
    "    X_feat[\"pay_amt_sep\"] / X_feat[\"bill_sep\"].replace(0, 1)\n",
    ").clip(upper=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19d072",
   "metadata": {},
   "source": [
    "Here we have removed raw colums from X6 to X23 and we have also removed gender, education, marital_Status as they do not add any good predicting power instead they canmake model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caac54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"credit_limit\",\n",
    "    \"age\",\n",
    "\n",
    "    \"recent_delay\",\n",
    "    \"max_delay\",\n",
    "    \"mean_delay\",\n",
    "    \"num_delayed_months\",\n",
    "    \"any_severe_delay\",\n",
    "\n",
    "    \"avg_bill\",\n",
    "    \"avg_payment\",\n",
    "    \"payment_ratio\",\n",
    "    \"recent_payment_ratio\",\n",
    "    \"utilization_ratio\"\n",
    "]\n",
    "\n",
    "X_final = X_feat[features].copy()\n",
    "\n",
    "print(X_final.shape)\n",
    "X_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()*100\n",
    "# 22 % of customers default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ef374",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train, X_test, y_full_train, y_test = train_test_split(\n",
    "    X_final,\n",
    "    y,\n",
    "    test_size= 0.2,\n",
    "    stratify=y,\n",
    "    random_state=11\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full_train,\n",
    "    y_full_train,\n",
    "    test_size= 0.25,\n",
    "    random_state=11,\n",
    "    stratify= y_full_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4267d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train), type(y_val), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing y to a numpy array for best results cus we need 1D target vectors.\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_val   = y_val.values.ravel()\n",
    "y_test  = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_val_proba = lr.predict_proba(X_val)[:, 1]\n",
    "y_train_proba = lr.predict_proba(X_train)[:, 1]\n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "val_auc   = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "train_auc, val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a56030",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_final.columns.tolist()\n",
    "\n",
    "coef = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": lr.coef_[0]\n",
    "}).sort_values(by=\"coef\", ascending=False)\n",
    "\n",
    "coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13dfc34",
   "metadata": {},
   "source": [
    "The strongest features of risk of defaults were behavioural features such as delay in payments.\n",
    "Age has mild positive risk as older people have slight chance to default.\n",
    "Credit limit has negative coef as higher the credit limit more buffer and more trust between bank and customer.\n",
    "Average_payment is also negative as people who pay regularly are safer.\n",
    "payment_ratio overlaps with avg_payment.\n",
    "recent_payment_ratio also also lowers risk of default.\n",
    "\n",
    "Here mean_delay is slightly negative, it could be due to as this is a multivariate model and standalone features do not add meaning.\n",
    "any_severe_delay also got captured in max_delay.\n",
    "\n",
    "Here we will drop some unnecessaery features like any_severe_delay as it is redundant with max_delay and we will also drop mean_delay as it is collinear to few dealy features so it doesnt add anything new to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = [\n",
    "    \"credit_limit\",\n",
    "    \"age\",\n",
    "    \"recent_delay\",\n",
    "    \"max_delay\",\n",
    "    \"num_delayed_months\",\n",
    "    \"avg_bill\",\n",
    "    \"avg_payment\",\n",
    "    \"payment_ratio\",\n",
    "    \"recent_payment_ratio\",\n",
    "    \"utilization_ratio\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_val_df   = pd.DataFrame(X_val,   columns=feature_names)\n",
    "X_test_df  = pd.DataFrame(X_test,  columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22293ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train_df[new_features]\n",
    "X_val_df = X_val_df[new_features]\n",
    "X_test_df = X_test_df[new_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_names = X_train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scaler = StandardScaler()\n",
    "\n",
    "X_train_p = final_scaler.fit_transform(X_train_df)\n",
    "X_val_p   = final_scaler.transform(X_val_df)\n",
    "X_test_p  = final_scaler.transform(X_test_df)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_p, y_train)\n",
    "\n",
    "y_train_proba_1 = lr.predict_proba(X_train_p)[:,1]\n",
    "y_val_proba_1 = lr.predict_proba(X_val_p)[:,1]\n",
    "\n",
    "train_auc = roc_auc_score(y_train, lr.predict_proba(X_train_p)[:,1])\n",
    "val_auc   = roc_auc_score(y_val,   lr.predict_proba(X_val_p)[:,1])\n",
    "\n",
    "train_auc, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaating Risk Buckets:\n",
    "\n",
    "risk = pd.DataFrame({\n",
    "    \"y_true\": y_val,\n",
    "    \"y_score\": y_val_proba_1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk[\"risk_bucket\"] = pd.qcut(\n",
    "    risk[\"y_score\"],\n",
    "    q=5,\n",
    "    labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970644f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk.groupby(\"risk_bucket\")[\"y_true\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73296a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,          # IMPORTANT: keep shallow\n",
    "    min_samples_leaf=200, # stabilizes probabilities\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_p, y_train)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, rf.predict_proba(X_train_p)[:,1])\n",
    "val_auc   = roc_auc_score(y_val, rf.predict_proba(X_val_p)[:,1])\n",
    "\n",
    "train_auc, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53247482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb.fit(X_train_p, y_train)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, gb.predict_proba(X_train_p)[:,1])\n",
    "val_auc   = roc_auc_score(y_val, gb.predict_proba(X_val_p)[:,1])\n",
    "\n",
    "train_auc, val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1c8ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Here gradient boosting is capturing non linear patters but not genralising that's why there is diffrence in trian and val auc.\n",
    "# Gradient Boosting is overfillting here as after increasing n_estimators and decreasing learning rate still it overfits and this means \n",
    "# celling has reached now further if we use gb it will learn noise/small patterns and overfit.\n",
    "\n",
    "# Here we should stop as train auc keeps on increasing by tuning hyperparameters but val auc remains constant or drops slightly. \n",
    "# so random forest will be final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7221922",
   "metadata": {},
   "source": [
    "#### Final Model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,          # IMPORTANT: keep shallow\n",
    "    min_samples_leaf=200, # stabilizes probabilities\n",
    "    random_state=1,\n",
    "    n_jobs = -1           # n_jobs = -1 (use all available CPU cores)\n",
    ")\n",
    "\n",
    "rf.fit(X_train_p, y_train)\n",
    "\n",
    "y_train_proba_2 = rf.predict_proba(X_train_p)[:,1]\n",
    "y_val_proba_2 = rf.predict_proba(X_val_p)[:,1]\n",
    "\n",
    "train_auc = roc_auc_score(y_train, rf.predict_proba(X_train_p)[:,1])\n",
    "val_auc   = roc_auc_score(y_val, rf.predict_proba(X_val_p)[:,1])\n",
    "\n",
    "train_auc, val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaating Risk Buckets:\n",
    "\n",
    "risk = pd.DataFrame({\n",
    "    \"y_true\": y_val,\n",
    "    \"y_score\": y_val_proba_2\n",
    "})\n",
    "\n",
    "risk[\"risk_bucket\"] = pd.qcut(\n",
    "    risk[\"y_score\"],\n",
    "    q=5,\n",
    "    labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    ")\n",
    "\n",
    "risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = rf.predict_proba(X_test_p)[:, 1]\n",
    "\n",
    "risk_test = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_score\": y_test_proba\n",
    "})\n",
    "\n",
    "risk_test[\"risk_bucket\"] = pd.qcut(\n",
    "    risk_test[\"y_score\"],\n",
    "    q=5,\n",
    "    labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    ")\n",
    "\n",
    "risk_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_test.groupby(\"risk_bucket\")[\"y_true\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
